extracted = str_match(pos, '([가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
len.analyzer = length(extracted[,1])
## 추출 키워드 통합
if (len.analyzer == 0) {
keyword = "none"
return(keyword)
}
else {
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
return(keyword)
}
}
MorphAnalyzer(s7_df[10])
x = s7_df[10]
sentence = as.character(x)
sentence = gsub('[-,~?!.]', ' ', sentence)
pos = paste(MorphAnalyzer(sentence))
extracted = str_match(pos, '([가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
extracted = str_match(pos, '([ㄱ-ㅎ가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
len.analyzer = length(extracted[,1])
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
return(keyword)
keyword = gsub('^', ' ', keyword)
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('^', ' ', keyword)
keyword = paste(keyword, collapse = " ")
keyword = gsub('^', '', keyword)
keyword = paste(keyword, collapse = " ")
keyword = keyword[!is.na(keyword)]
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('^', '', keyword)
keyword = gsub('\^', '', keyword)
keyword = gsub('\^\', '', keyword)
keyword
ko_tokenizer = function(x) {
sentence = as.character(x)
sentence = gsub('[-,~?!.]', ' ', sentence)
pos = paste(MorphAnalyzer(sentence))
## 품사별 추출
extracted = str_match(pos, '([가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
len.analyzer = length(extracted[,1])
## 추출 키워드 통합
if (len.analyzer == 0) {
keyword = "none"
return(keyword)
}
else {
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
return(keyword)
}
}
tokenize = function(df) {
filtered_df = list()
count = 0
for (i in seq(df)) {
filtered = iconv(ko_tokenizer(df[i]), "UTF-8", localeToCharset()[1])
filtered_df = append(filtered, filtered_df)
count = count + 1
cat('Tokenizer is working... ', count, (count/nrow(df))*100, '% completed', '\n')
}
return(filtered_df)
}
paral_tokenize = function(df) {
#count = 0
filtered_df = foreach(i = 1:length(df),
.combine = rbind,
.errorhandling='pass',
.packages = c('KoNLP', 'stringr'),
.export = 'ko_tokenizer') %dopar% {
#filtered = iconv(ko_tokenizer(df[i]), localeToCharset()[1], "UTF-8")
filtered = ko_tokenizer(df[i])
#count = count + 1
#cat('Tokenizer is working... ', count, (count/nrow(df))*100, '% completed', '\n')
return(filtered)
}
}
raw_data = read_csv('D:/works/project/phone_csf/dataframes/phone_df.csv')
s7_raw = subset(raw_data, select=c("title", "content"), product=="s7")
s7_content = paste(s7_raw$title, s7_raw$content)
s7_ilbe = read_csv('D:/works/project/phone_csf/dataframes/ilbe_s7.csv')
s7_ilbe_content = paste(s7_ilbe$title, s7_ilbe$content)
s7_df = c(s7_content, s7_ilbe_content)
df = tokenize(s7_df)
keyword = gsub('[^]', '', keyword)
keyword = gsub('[^^]', '', keyword)
keyword = gsub('[^]', '', keyword)
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('[^]', '', keyword)
keyword = gsub('[^.]', '', keyword)
keyword = paste(keyword, collapse = " ")
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('[^{1}]', '', keyword)
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('[^(1)]', '', keyword)
print('^')
pos = paste(MorphAnalyzer(sentence))
extracted = str_match(pos, '([ㄱ-ㅎ가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
pos = paste(MorphAnalyzer(sentence))
sentence = gsub('[-,~?!.]', ' ', sentence)
pos = paste(MorphAnalyzer(sentence))
extracted = str_match(pos, '([ㄱ-ㅎ가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
len.analyzer = length(extracted[,1])
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('(^)', '', keyword)
keyword = gsub(''^'', '', keyword)
keyword
keyword = gsub('^', '', keyword)
keyword = gsub('[^]', '', keyword)
keyword = gsub('[^] ', '', keyword)
keyword = gsub(' [^]', ' ', keyword)
keyword = gsub('[^]', ' ', keyword)
keyword = gsub('[^+1]', ' ', keyword)
keyword = paste(keyword, collapse = " ")
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('[^]{1}', ' ', keyword)
keyword = gsub('([^]{1})', ' ', keyword)
keyword = gsub('([^]]{1})', ' ', keyword)
keyword = gsub('[^]]{1}', ' ', keyword)
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('[^]]{1}', ' ', keyword)
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
extracted = str_match(pos, '([ㄱ-ㅎ가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
len.analyzer = length(extracted[,1])
pos = paste(MorphAnalyzer(sentence))
sentence = gsub('[-,~?!.]', ' ', sentence)
sentence = as.character(x)
sentence = gsub('[-,~?!.]', ' ', sentence)
pos = paste(MorphAnalyzer(sentence))
keyword = gsub('[^]{1}', ' ', keyword)
keyword = gsub('[^{1}]', ' ', keyword)
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('[^{1}]', ' ', keyword)
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('[^{0}]', '', keyword)
keyword = gsub('^', 'a', keyword)
sentence = as.character(x)
sentence = gsub('[-,~?!.]', ' ', sentence)
pos = paste(MorphAnalyzer(sentence))
extracted = str_match(pos, '([ㄱ-ㅎ가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
len.analyzer = length(extracted[,1])
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('^', 'a', keyword)
keyword = paste(keyword, collapse = " ")
keyword = keyword[!is.na(keyword)]
len.analyzer = length(extracted[,1])
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('^', 'a', keyword)
keyword = gsub('^^', 'a', keyword)
keyword = gsub('^^', 'a', keyword)
keyword = gsub('^^', 'a', keyword)
keyword = gsub('^^^', 'a', keyword)
keyword = gsub('^', 'a', keyword)
keyword = gsub('^', 'a', keyword)
len.analyzer = length(extracted[,1])
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('^', '', as.character(keyword))
keyword = gsub('^', '', as.character(keyword))
keyword = gsub('^', '', str(keyword))
len.analyzer = length(extracted[,1])
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('^', '', str(keyword))
len.analyzer = length(extracted[,1])
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = str_replace(keyword, '^', '')
keyword = str_replace(keyword, '^^', '')
keyword = str_replace(keyword, '[^]', '')
keyword = str_replace(keyword, '[^ ]', '')
keyword = str_replace(keyword, '[^ ]', '')
sentence = as.character(x)
sentence = gsub('[-,~?!.]', ' ', sentence)
pos = paste(MorphAnalyzer(sentence))
extracted = str_match(pos, '([ㄱ-ㅎ가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
len.analyzer = length(extracted[,1])
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = str_replace(keyword, '[ ^ ]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
keyword = str_replace(keyword, '[ ^]', '')
sentence = as.character(x)
sentence = gsub('[-,~?!.]', ' ', sentence)
pos = paste(MorphAnalyzer(sentence))
extracted = str_match(pos, '([ㄱ-ㅎ가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
len.analyzer = length(extracted[,1])
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = str_replace(keyword, '[ ^^]', '')
keyword = str_replace(keyword, ' ^', '')
keyword = str_replace(keyword, ' ^', '')
keyword = str_replace(keyword, ' ', '')
keyword = str_replace(keyword, ' ', '')
keyword = str_replace(keyword, ' ', '')
keyword = str_replace(keyword, ' ', '')
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = str_replace(keyword, ' ', '')
len.analyzer = length(extracted[,1])
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = str_replace(keyword, '[:^:]', '')
keyword = str_replace(keyword, '[:^:]', '')
keyword = str_replace(keyword, '[:^:]', '')
keyword = str_replace(keyword, '[:^:]', '')
keyword = str_replace(keyword, '[:^:]', '')
keyword = str_replace(keyword, '[:^:]', '')
keyword = str_replace(keyword, '[:^:]', '')
keyword = str_replace(keyword, '[:^:]', '')
keyword = str_replace(keyword, '[:^:]', '')
keyword = str_replace(keyword, '[:^:]', '')
len.analyzer = length(extracted[,1])
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('[:^:]', '', keyword)
library(KoNLP)
library(tm)
library(wordcloud)
library(stringr)
library(readr)
library(foreach)
library(doParallel)
#Sys.setlocale("LC_ALL", "en_US.UTF-8")
#Sys.setenv(LANG = "en_US.UTF-8")
#Sys.setlocale("LC_ALL","English")
#options(encoding = 'UTF-8')
cl <- makeCluster(4)
registerDoParallel(cl)
useNIADic()
setwd("d:/works")
ko_tokenizer = function(x) {
sentence = as.character(x)
sentence = gsub('[-,~?!.]', ' ', sentence)
pos = paste(MorphAnalyzer(sentence))
## 품사별 추출
extracted = str_match(pos, '([가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
len.analyzer = length(extracted[,1])
## 추출 키워드 통합
if (len.analyzer == 0) {
keyword = "none"
return(keyword)
}
else {
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('[:^:]', '', keyword)
return(keyword)
}
}
tokenize = function(df) {
filtered_df = list()
count = 0
for (i in seq(df)) {
filtered = iconv(ko_tokenizer(df[i]), "UTF-8", localeToCharset()[1])
filtered_df = append(filtered, filtered_df)
count = count + 1
cat('Tokenizer is working... ', count, (count/nrow(df))*100, '% completed', '\n')
}
return(filtered_df)
}
paral_tokenize = function(df) {
#count = 0
filtered_df = foreach(i = 1:length(df),
.combine = rbind,
.errorhandling='pass',
.packages = c('KoNLP', 'stringr'),
.export = 'ko_tokenizer') %dopar% {
#filtered = iconv(ko_tokenizer(df[i]), localeToCharset()[1], "UTF-8")
filtered = ko_tokenizer(df[i])
#count = count + 1
#cat('Tokenizer is working... ', count, (count/nrow(df))*100, '% completed', '\n')
return(filtered)
}
}
raw_data = read_csv('D:/works/project/phone_csf/dataframes/phone_df.csv')
s7_raw = subset(raw_data, select=c("title", "content"), product=="s7")
s7_content = paste(s7_raw$title, s7_raw$content)
s7_ilbe = read_csv('D:/works/project/phone_csf/dataframes/ilbe_s7.csv')
s7_ilbe_content = paste(s7_ilbe$title, s7_ilbe$content)
s7_df = c(s7_content, s7_ilbe_content)
a = paral_tokenize(s7_df[1:10])
View(a)
ko_tokenizer = function(x) {
sentence = as.character(x)
sentence = gsub('[-,~?!.]', ' ', sentence)
pos = paste(MorphAnalyzer(sentence))
## 품사별 추출
extracted = str_match(pos, '([가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
len.analyzer = length(extracted[,1])
## 추출 키워드 통합
if (len.analyzer == 0) {
keyword = "none"
return(keyword)
}
else {
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('[:^:]', '', keyword)
return(keyword)
}
}
a = paral_tokenize(s7_df[1:10])
View(a)
View(ko_tokenizer)
ko_tokenizer = function(x) {
sentence = as.character(x)
sentence = gsub('[-,~?!.]', ' ', sentence)
pos = paste(MorphAnalyzer(sentence))
## 품사별 추출
extracted = str_match(pos, '([ㄱ-ㅎ가-힣A-z]+)/(f|ncn|ncpa|ncps|paa)')
len.analyzer = length(extracted[,1])
## 추출 키워드 통합
if (len.analyzer == 0) {
keyword = "none"
return(keyword)
}
else {
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('[:^:]', '', keyword)
return(keyword)
}
}
paral_tokenize = function(df) {
#count = 0
filtered_df = foreach(i = 1:length(df),
.combine = rbind,
.errorhandling='pass',
.packages = c('KoNLP', 'stringr'),
.export = 'ko_tokenizer') %dopar% {
#filtered = iconv(ko_tokenizer(df[i]), localeToCharset()[1], "UTF-8")
filtered = ko_tokenizer(df[i])
#count = count + 1
#cat('Tokenizer is working... ', count, (count/nrow(df))*100, '% completed', '\n')
#return(filtered)
}
}
a = paral_tokenize(s7_df[1:10])
View(a)
paral_tokenize = function(df) {
#count = 0
filtered_df = foreach(i = 1:length(df),
.combine = rbind,
.errorhandling='pass',
.packages = c('KoNLP', 'stringr'),
.export = 'ko_tokenizer') %dopar% {
#filtered = iconv(ko_tokenizer(df[i]), localeToCharset()[1], "UTF-8")
filtered = ko_tokenizer(df[i])
#count = count + 1
cat('Tokenizer is working... ', count, (count/nrow(df))*100, '% completed', '\n')
#return(filtered)
}
}
a = paral_tokenize(s7_df[1:10])
paral_tokenize = function(df) {
#count = 0
filtered_df = foreach(i = 1:length(df),
.combine = rbind,
.errorhandling='pass',
.packages = c('KoNLP', 'stringr'),
.export = 'ko_tokenizer') %dopar% {
#filtered = iconv(ko_tokenizer(df[i]), localeToCharset()[1], "UTF-8")
filtered = ko_tokenizer(df[i])
#count = count + 1
cat('Tokenizer is working... ')
#return(filtered)
}
}
a = paral_tokenize(s7_df[1:10])
paral_tokenize = function(df) {
filtered_df = foreach(i = 1:length(df),
.combine = rbind,
.errorhandling='pass',
.packages = c('KoNLP', 'stringr'),
.export = 'ko_tokenizer') %dopar% {
filtered = ko_tokenizer(df[i])
}
}
a = paral_tokenize(s7_df[1:10])
df = paral_tokenize(s7_df)
View(df)
save.image("D:/works/phone_csf_db.RData")
df[1:10]
write.csv(df, file="pcsf_pre_dataframe.csv", row.names = F, col.names = F)
load("D:/works/project/phone_csf/phone_csf_db.RData")
tokenize = function(df) {
filtered_df = list()
count = 0
for (i in seq(df)) {
filtered = iconv(ko_tokenizer(df[i]), "UTF-8", localeToCharset()[1])
filtered_df = append(filtered, filtered_df)
count = count + 1
cat('Tokenizer is working... ', count, (count/length(df))*100, '% completed', '\n')
}
return(filtered_df)
}
df1 = tokenize(s7_df)
# Library 로딩
library(KoNLP)
library(tm)
library(wordcloud)
library(stringr)
library(readr)
library(foreach)
library(doParallel)
df1 = tokenize(s7_df)
# Library 로딩
library(KoNLP)      # 한국어 품사태깅
library(stringr)    # R 정규표현식
library(readr)      # DataFrame reader
library(foreach)    # 병렬처리 반복문
library(doParallel) # 멀티코어 병렬처리 지원
#cl <- makeCluster(4)    # 멀티코어 개수 지정
#registerDoParallel(cl)  # 병렬처리에 등록
useNIADic() # NIADic 사용
setwd("d:/works/project/phone_csf") # 작업경로 지정
# 단어 POS 태깅용 함수
ko_tokenizer = function(x) {
sentence = as.character(x)
sentence = gsub('[-,~?!.&;]', ' ', sentence)
pos = paste(MorphAnalyzer(sentence))
## 품사별 추출
extracted = str_match(pos, '([ㄱ-ㅎ가-힣A-z0-9]+)/(paa|ncps|pvg)')
len.analyzer = length(extracted[,1])
## 추출 키워드 통합
if (len.analyzer == 0) {
keyword = "none"
return(keyword)
}
else {
keyword = c(extracted[1:len.analyzer,2])
keyword = keyword[!is.na(keyword)]
keyword = paste(keyword, collapse = " ")
keyword = gsub('[:^:]', '', keyword)
return(keyword)
}
}    # 일반단어 추출용
kotok = function(df) {
filtered_df = list()
count = 0
for (i in seq(df)) {
filtered = ko_tokenizer(df[i])
filtered_df = append(filtered, filtered_df)
count = count + 1
cat('Tokenizer is working... ', count, (count/length(df))*100, '% completed', '\n')
}
return(filtered_df)
}  # 문서 일반단어 추출
raw_data = read_delim('D:/works/project/phone_csf/dataframes/phone_fullframe.csv', '\t') # DataFrame 로딩
phone_text = subset(raw_data, select=c("title", "content")) # DataFrame 제목, 내용 병합
phone_text = paste(phone_text$title, phone_text$content)
ko_df = as.matrix(kotok(phone_text))
write.csv(ko_df, file="pcsf_fullframe2.csv", row.names = F, col.names = F)
MorphAnalyzer("투명")
MorphAnalyzer("투명하")
MorphAnalyzer("액정에 흠집")
