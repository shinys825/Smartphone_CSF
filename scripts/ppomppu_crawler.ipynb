{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "class Ppomppu(object):\n",
    "    def __init__(self, date_from, date_to, keyword, start_page):\n",
    "        self.date_from = date_from\n",
    "        self.date_to = date_to\n",
    "        self.keyword = keyword\n",
    "        self.start_page = start_page\n",
    "    \n",
    "    # on Board\n",
    "    def crawl(self):\n",
    "        f = codecs.open('ppomppu_{}.txt'.format(self.keyword), 'a', 'utf-8')\n",
    "        page_num = self.start_page\n",
    "        num_contents = 0\n",
    "        \n",
    "        while True:\n",
    "            hdr = {'User-Agent' : 'Mozilla/5.0',\n",
    "                   'referer' : 'https://ppomppu.co.kr/'}\n",
    "            url = \"http://www.ppomppu.co.kr/search_bbs.php?search_type=subject&page_no={0}&keyword={1}&page_size=20&bbs_id=&order_type=date&bbs_cate=2\".format(page_num, self.keyword)\n",
    "            res = requests.get(url, headers = hdr)\n",
    "            soup = BeautifulSoup(res.content, 'html5')\n",
    "            links = soup.find_all('span', attrs={'class': 'title'})\n",
    "            \n",
    "            date_soup = soup.find_all('p', attrs={'class': 'desc'})\n",
    "            \n",
    "            try:\n",
    "                raw_date = re.search(r'\\d{4}.\\d{2}', date_soup[1].get_text()).group()\n",
    "                chk_date = raw_date.replace('.', '')\n",
    "            except AttributeError:\n",
    "                raw_date = re.search(r'\\d{4}.\\d{2}', date_soup[2].get_text()).group()\n",
    "                chk_date = raw_date.replace('.', '')\n",
    "\n",
    "            print(page_num)\n",
    "            page_num += 1\n",
    "\n",
    "            if int(chk_date) <= int(self.date_to) and int(chk_date) > int(self.date_from):\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        link = link.a['href']\n",
    "                        res2 = requests.get(link, headers = hdr)\n",
    "                        soup2 = BeautifulSoup(res2.content, 'html5')\n",
    "                        contents = soup2.find_all('div', attrs={'class': 'container'})\n",
    "                        num_contents += 1\n",
    "                        print('processing... ' + str(num_contents) + ' ' + str(link))\n",
    "\n",
    "                    \n",
    "                        for form in contents:\n",
    "                            try:\n",
    "                                container = form.select('td.han')[1].get_text()\n",
    "                                date = re.search(r'\\d{4}-\\d{2}-\\d{2}', container).group()\n",
    "                                view = re.search(r'\\d+\\s/', container).group()[0:-2]\n",
    "                                title = form.select('font.view_title2')[0].get_text()\n",
    "                                content = re.sub(r'\\s+', ' ', form.select('td.board-contents')[0].get_text())\n",
    "                                time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "                                f.write(link+'\\t'+date+'\\t'+view+'\\t'+title+'\\t'+content+'\\t'+time+'\\n')\n",
    "                            except IndexError:\n",
    "                                pass\n",
    "                    except TypeError:\n",
    "                        pass\n",
    "                    \n",
    "            elif int(chk_date) == int(self.date_from):\n",
    "                print('Reached to date limit ('+str(self.date_to)+', yyyy-mm)')\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "        f.close()\n",
    "        print(\"Contents crawling has been completed, total \" + str(num_contents) + ' contents')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-fa01cc0d8510>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#p_g5 = Ppomppu(201512, 201607, 'g5', 566)\n",
    "#p_g5.crawl()\n",
    "\n",
    "p_g6 = Ppomppu(201703, 201705, 'g6', 1)\n",
    "p_g6.crawl()\n",
    "\n",
    "#p_s6 = Ppomppu(201412, 201507, 's6', 1)\n",
    "#p_s6.crawl()\n",
    "\n",
    "#p_s7 = Ppomppu(201512, 201607, 's7', 42)\n",
    "#p_s7.crawl()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
