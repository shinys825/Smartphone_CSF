{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "import threading\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "class Ilbe(object):\n",
    "    def __init__(self, date_from, date_to, keyword, start_page):\n",
    "        self.date_from = date_from\n",
    "        self.date_to = date_to\n",
    "        self.keyword = keyword\n",
    "        self.start_page = start_page\n",
    "\n",
    "    def str_process(self, string):\n",
    "        #del_blank = re.compile(r'(\\s+)')\n",
    "        del_escape = re.compile(r'(\\n)(|\\t)')\n",
    "        #string = del_blank.sub(' ', string)\n",
    "        string = del_escape.sub('', string)\n",
    "        return string\n",
    "        \n",
    "    # on Board\n",
    "    def crawl(self):\n",
    "        f = codecs.open('ilbe_{}.txt'.format(self.keyword), 'a', 'utf-8')\n",
    "        page_num = self.start_page\n",
    "        #view_flag = -1\n",
    "        num_contents = 0\n",
    "        \n",
    "        while True:\n",
    "            hdr = {'User-Agent' : 'Mozilla/5.0'}\n",
    "            url = \"http://www.ilbe.com/index.php?mid=smartphone&search_target=title_content&search_keyword={0}&page={1}\".format(self.keyword, self.start_page)\n",
    "            res = requests.get(url, headers = hdr)\n",
    "            soup = BeautifulSoup(res.content, 'lxml')\n",
    "            links = soup.find_all('td', attrs={'class': 'title'})\n",
    "\n",
    "            date_soup = soup.find_all('td', attrs={'class': 'date'})\n",
    "            \n",
    "            try:\n",
    "                if ':' in date_soup[3].get_text():\n",
    "                    chk_date = datetime.datetime.today().strftime('%Y%m')\n",
    "                else:\n",
    "                    raw_date = re.search(r'\\d{4}-\\d{2}', date_soup[3].get_text()).group()\n",
    "                    chk_date = raw_date.replace('-', '')\n",
    "            except IndexError:\n",
    "                pass\n",
    "                    \n",
    "            print(self.start_page)\n",
    "            self.start_page += 1\n",
    "            #view_flag += 1\n",
    "            threads = []\n",
    "\n",
    "            if int(chk_date) <= int(self.date_to) and int(chk_date) > int(self.date_from):\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        link = link.a['href']\n",
    "                        res2 = requests.get(link, headers = hdr)\n",
    "                        soup2 = BeautifulSoup(res2.content, 'lxml')\n",
    "                        contents = soup2.find_all('div', attrs={'class': 'originalContent'})\n",
    "                        \n",
    "                        if link[-9:] == '214481493' or link[-10:] == '1877136960':\n",
    "                            pass\n",
    "                        \n",
    "                        else:\n",
    "                            #view_index = num_contents - view_flag * 22\n",
    "                            num_contents += 1\n",
    "                            print('processing... ' + str(num_contents) + ' ' + str(link))\n",
    "                            try:\n",
    "                                date = self.str_process(contents[0].select('div.date')[0].get_text()[0:11].replace('.', '-'))\n",
    "                            except IndexError:\n",
    "                                pass\n",
    "                            try:\n",
    "                                #view = self.str_process(soup2.select('td.reading')[view_index].get_text())\n",
    "                                #print(num_contents,'-',view_flag,'*',view_index)\n",
    "                                title = self.str_process(contents[0].select('div.title')[0].get_text())\n",
    "                                content = self.str_process(contents[0].select('div#copy_layer_1')[0].get_text())\n",
    "                                c_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "                                f.write(link+'\\t'+date+'\\t'+title+'\\t'+content+'\\t'+c_time+'\\n')\n",
    "                            except IndexError:\n",
    "                                pass\n",
    "                    except TypeError:\n",
    "                        pass\n",
    "                    \n",
    "            elif int(chk_date) == int(self.date_from):\n",
    "                print('Reached to date limit ('+str(self.date_to)+', yyyy-mm)')\n",
    "                break\n",
    "            else:\n",
    "                num_contents += 1\n",
    "                pass\n",
    "        f.close()\n",
    "        print(\"Contents crawling has been completed, total \" + str(num_contents) + ' contents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i_g5 = Ilbe(201512, 201607, 'g5', 1)\n",
    "i_g5.crawl()\n",
    "\n",
    "#i_g6 = Ilbe(201612, 201703, 'g6', 46)\n",
    "#i_g6.crawl()\n",
    "\n",
    "#i_s6 = Ilbe(201412, 201507, 's6', 1)\n",
    "#i_s6.crawl()\n",
    "\n",
    "#i_s7 = Ilbe(201512, 201607, 's7', 455)\n",
    "#i_s7.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fa01cc0d8510>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "link = \"http://www.ilbe.com/index.php?mid=smartphone&search_target=title_content&search_keyword=g5&page=10&document_srl=9378278766\"\n",
    "res2 = requests.get(link)\n",
    "soup2 = BeautifulSoup(res2.content, 'lxml')\n",
    "contents = soup2.find_all('div', attrs={'class': 'originalContent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def str_process(string):\n",
    "    #del_blank = re.compile(r'(\\s+)')\n",
    "    del_escape = re.compile(r'(\\n)(|\\t)')\n",
    "    #string = del_blank.sub(' ', string)\n",
    "    string = del_escape.sub('', string)\n",
    "    return string\n",
    "\n",
    "date = str_process(contents[0].select('div.date')[0].get_text()[0:11].replace('.', '-'))\n",
    "title = str_process(contents[0].select('div.title')[0].get_text())\n",
    "content = str_process(contents[0].select('div#copy_layer_1')[0].get_text())\n",
    "\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from threading import Thread\n",
    "from threading import Lock\n",
    "from Queue import Queue\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "class Ilbe(object):\n",
    "    def __init__(self, date_from, date_to, keyword, start_page):\n",
    "        self.date_from = date_from\n",
    "        self.date_to = date_to\n",
    "        self.keyword = keyword\n",
    "        self.start_page = start_page\n",
    "        \n",
    "        self.hdr = {'User-Agent' : 'Mozilla/5.0'}\n",
    "        self.view_flag = 0\n",
    "        self.num_contents = 0\n",
    "        \n",
    "        self.lock = Lock()\n",
    "        self.n = 0\n",
    "        \n",
    "        self.f = codecs.open('ilbe_{}.txt'.format(self.keyword), 'a', 'utf-8')\n",
    "\n",
    "    def str_process(self, string):\n",
    "        del_escape = re.compile(r'(\\n)(|\\t)')\n",
    "        string = del_escape.sub('', string)\n",
    "        return string\n",
    "    \n",
    "    def get_contents(self, link):\n",
    "        link = link\n",
    "        print(link)\n",
    "        res = requests.get(link, headers = self.hdr)\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        contents = soup.find_all('div', attrs={'class': 'originalContent'})\n",
    "\n",
    "        if link[-9:] == '214481493' or link[-10:] == '1877136960':\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            view_index = self.num_contents - self.view_flag * 22\n",
    "            self.num_contents += 1\n",
    "            #print('processing... ' + str(self.num_contents) + ' ' + str(link))\n",
    "            print(contents[0].select('div.date')[0].get_text()[0:11].replace('.', '-'))\n",
    "\n",
    "            #date = self.str_process(contents[0].select('div.date')[0].get_text()[0:11].replace('.', '-'))\n",
    "            #view = self.str_process(soup.select('td.reading')[view_index].get_text())\n",
    "            #title = self.str_process(contents[0].select('div.title')[0].get_text())\n",
    "            #content = self.str_process(contents[0].select('div#copy_layer_1')[0].get_text())\n",
    "            #time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "            #self.f.write(link+'\\t'+date+'\\t'+view+'\\t'+title+'\\t'+content+'\\t'+time+'\\n')\n",
    "\n",
    "    # on Board\n",
    "    def crawl(self):\n",
    "        while True:\n",
    "            url = \"http://www.ilbe.com/index.php?mid=smartphone&search_target=title_content&search_keyword={0}&page={1}\".format(self.keyword, self.start_page)\n",
    "            res = requests.get(url, headers = self.hdr)\n",
    "            soup = BeautifulSoup(res.content, 'lxml')\n",
    "            links = soup.find_all('td', attrs={'class': 'title'})\n",
    "\n",
    "            date_soup = soup.find_all('td', attrs={'class': 'date'})\n",
    "            \n",
    "            if ':' in date_soup[2].get_text():\n",
    "                chk_date = datetime.datetime.today().strftime('%Y%m')\n",
    "            else:\n",
    "                raw_date = re.search(r'\\d{4}-\\d{2}', date_soup[3].get_text()).group()\n",
    "                chk_date = raw_date.replace('-', '')\n",
    "\n",
    "            print(self.start_page)\n",
    "            self.start_page += 1\n",
    "            self.view_flag += 1\n",
    "            \n",
    "            threads = []\n",
    "\n",
    "            if int(chk_date) <= int(self.date_to) and int(chk_date) > int(self.date_from):\n",
    "                for link in links:\n",
    "                    link = link.a['href']\n",
    "                    \n",
    "                    t = threading.Thread(target=self.get_contents, args=(link,))\n",
    "\n",
    "                    with self.lock:\n",
    "                        self.n += 1\n",
    "                        \n",
    "                    t.start()\n",
    "                    \n",
    "                for t in threads:\n",
    "                    t.join()\n",
    "                    \n",
    "            elif int(chk_date) == int(self.date_from):\n",
    "                print('Reached to date limit ('+str(self.date_to)+', yyyy-mm)')\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "        self.f.close()\n",
    "        print(\"Contents crawling has been completed, total \" + str(self.num_contents) + ' contents')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-68e9b78cf7eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mi_g5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIlbe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m201702\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m201703\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'g5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mi_g5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-fb8c8f782a15>\u001b[0m in \u001b[0;36mcrawl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mdate_soup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'date'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;34m':'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdate_soup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m                 \u001b[0mchk_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y%m'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "i_g5 = Ilbe(201702, 201703, 'g5', 4)\n",
    "i_g5.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2016-04-10'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://www.ilbe.com/index.php?mid=smartphone&search_target=title_content&search_keyword=s7&page=455&document_srl=7855756735\"\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, 'lxml')\n",
    "links = soup.find_all('td', attrs={'class': 'title'})\n",
    "\n",
    "date_soup = soup.find_all('td', attrs={'class': 'date'})\n",
    "date_soup[2].get_text()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
